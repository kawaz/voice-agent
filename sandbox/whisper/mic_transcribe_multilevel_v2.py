#!/usr/bin/env python3
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os
import time
import threading
import queue
import multiprocessing as mp
from collections import deque
from dataclasses import dataclass
from typing import List, Tuple, Optional

@dataclass
class AudioChunk:
    """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    audio: np.ndarray
    timestamp: float
    duration: float
    level: str
    rms: float  # éŸ³é‡ãƒ¬ãƒ™ãƒ«

class AudioAnalyzer:
    """éŸ³å£°åˆ†æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    @staticmethod
    def calculate_rms(audio_data: np.ndarray) -> float:
        """RMSï¼ˆéŸ³é‡ï¼‰ã‚’è¨ˆç®—"""
        if len(audio_data) == 0:
            return 0
        return np.sqrt(np.mean(audio_data.astype(np.float32)**2))
    
    @staticmethod
    def is_speech(audio_data: np.ndarray, threshold: float = 300) -> bool:
        """éŸ³å£°ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        rms = AudioAnalyzer.calculate_rms(audio_data)
        return rms > threshold
    
    @staticmethod
    def get_energy_ratio(audio_data: np.ndarray) -> float:
        """ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¯”ç‡ã‚’è¨ˆç®—ï¼ˆç„¡éŸ³éƒ¨åˆ†ã®å‰²åˆï¼‰"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
        frame_size = 1024
        energies = []
        
        for i in range(0, len(audio_data) - frame_size, frame_size):
            frame = audio_data[i:i+frame_size]
            energy = np.sum(frame.astype(np.float32)**2)
            energies.append(energy)
        
        if not energies:
            return 0
            
        # ä½ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰²åˆ
        threshold = np.percentile(energies, 30)
        low_energy_frames = sum(1 for e in energies if e < threshold)
        return low_energy_frames / len(energies)

class MultiLevelBuffer:
    """æ”¹è‰¯ç‰ˆãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ãƒãƒƒãƒ•ã‚¡"""
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        
        # å„ãƒ¬ãƒ™ãƒ«ã®è¨­å®šï¼ˆéŸ³å£°æ¤œå‡ºé–¾å€¤ã‚’è¿½åŠ ï¼‰
        self.levels = {
            'short': {
                'duration': 2.0, 
                'overlap': 0.5,
                'min_speech_ratio': 0.3  # 30%ä»¥ä¸ŠéŸ³å£°ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
            },
            'medium': {
                'duration': 5.0, 
                'overlap': 1.0,
                'min_speech_ratio': 0.2  # 20%ä»¥ä¸Š
            },
            'long': {
                'duration': 15.0, 
                'overlap': 2.0,
                'min_speech_ratio': 0.1  # 10%ä»¥ä¸Š
            }
        }
        
        self.buffers = {
            level: deque(maxlen=int(config['duration'] * sample_rate * 2))
            for level, config in self.levels.items()
        }
        
        self.last_processed = {level: 0 for level in self.levels}
        self.total_samples = 0
        
    def add_audio(self, audio_data: np.ndarray):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        for buffer in self.buffers.values():
            buffer.extend(audio_data)
        self.total_samples += len(audio_data)
        
    def get_chunks_to_process(self) -> List[AudioChunk]:
        """å‡¦ç†ã™ã¹ããƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆéŸ³å£°æ¤œå‡ºä»˜ãï¼‰"""
        chunks = []
        
        for level, config in self.levels.items():
            samples_needed = int(config['duration'] * self.sample_rate)
            samples_since_last = self.total_samples - self.last_processed[level]
            samples_step = samples_needed - int(config['overlap'] * self.sample_rate)
            
            if samples_since_last >= samples_step and len(self.buffers[level]) >= samples_needed:
                # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                audio_data = np.array(list(self.buffers[level])[-samples_needed:])
                
                # éŸ³å£°åˆ†æ
                rms = AudioAnalyzer.calculate_rms(audio_data)
                energy_ratio = AudioAnalyzer.get_energy_ratio(audio_data)
                speech_ratio = 1 - energy_ratio
                
                # éŸ³å£°ãŒååˆ†å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å‡¦ç†
                if speech_ratio >= config['min_speech_ratio'] and rms > 200:
                    chunk = AudioChunk(
                        audio=audio_data,
                        timestamp=time.time(),
                        duration=len(audio_data) / self.sample_rate,
                        level=level,
                        rms=rms
                    )
                    chunks.append(chunk)
                    self.last_processed[level] = self.total_samples
                else:
                    # ç„¡éŸ³ãŒå¤šã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãŸã ã—ã‚«ã‚¦ãƒ³ã‚¿ã¯æ›´æ–°ï¼‰
                    if level == 'short':  # shortãƒ¬ãƒ™ãƒ«ã®ã¿ãƒ­ã‚°è¡¨ç¤º
                        print(f"\rğŸ”‡ ç„¡éŸ³ã‚¹ã‚­ãƒƒãƒ— (éŸ³å£°æ¯”ç‡: {speech_ratio:.0%}, RMS: {rms:.0f})", end="", flush=True)
                    self.last_processed[level] = self.total_samples
                
        return chunks

class TranscriptionWorker(mp.Process):
    """æ”¹è‰¯ç‰ˆæ–‡å­—èµ·ã“ã—ãƒ¯ãƒ¼ã‚«ãƒ¼"""
    def __init__(self, model_name: str, input_queue: mp.Queue, output_queue: mp.Queue):
        super().__init__()
        self.model_name = model_name
        self.input_queue = input_queue
        self.output_queue = output_queue
        self.daemon = True
        
    def run(self):
        """ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        print(f"[Worker-{os.getpid()}] Whisperãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = whisper.load_model(self.model_name)
        print(f"[Worker-{os.getpid()}] ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
        
        # æ—¢çŸ¥ã®èª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        false_positives = [
            "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
            "å­—å¹•åˆ¶ä½œè€…",
            "æä¾›",
            "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
            "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™"
        ]
        
        while True:
            try:
                task = self.input_queue.get(timeout=1)
                
                if task is None:
                    break
                    
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—
                start_time = time.time()
                
                # éŸ³å£°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸè¨­å®š
                options = {
                    'language': 'ja',
                    'fp16': False,
                    'temperature': 0.0,  # ã‚ˆã‚Šæ±ºå®šçš„ãªå‡ºåŠ›
                    'compression_ratio_threshold': 2.4,  # åœ§ç¸®ç‡ãŒé«˜ã„å ´åˆã¯ç„¡åŠ¹
                    'logprob_threshold': -1.0,  # ä¿¡é ¼åº¦ã®ä½ã„çµæœã‚’é™¤å¤–
                    'no_speech_threshold': 0.6  # ç„¡éŸ³æ¤œå‡ºã®é–¾å€¤
                }
                
                # æ–‡è„ˆãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
                if task.get('prompt'):
                    options['initial_prompt'] = task['prompt']
                    
                result = model.transcribe(task['audio_file'], **options)
                transcribe_time = time.time() - start_time
                
                # çµæœã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                text = result['text'].strip()
                
                # æ—¢çŸ¥ã®èª¤èªè­˜ã‚’é™¤å¤–
                for fp in false_positives:
                    if fp in text and len(text) < len(fp) * 1.5:
                        text = ""
                        break
                
                # åœ§ç¸®ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜æ–‡å­—ã®ç¹°ã‚Šè¿”ã—ã‚’æ¤œå‡ºï¼‰
                if text and result.get('compression_ratio', 0) > 2.4:
                    text = ""
                    
                # çµæœã‚’è¿”ã™
                self.output_queue.put({
                    'text': text,
                    'segments': result.get('segments', []),
                    'level': task['level'],
                    'timestamp': task['timestamp'],
                    'duration': task['duration'],
                    'transcribe_time': transcribe_time,
                    'rms': task['rms'],
                    'no_speech_prob': result.get('no_speech_prob', 0)
                })
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if os.path.exists(task['audio_file']):
                    os.unlink(task['audio_file'])
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[Worker-{os.getpid()}] ã‚¨ãƒ©ãƒ¼: {e}")

class MultiLevelTranscriber:
    """æ”¹è‰¯ç‰ˆãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«éŸ³å£°èªè­˜"""
    def __init__(self, model_name="small", num_workers=2):
        self.model_name = model_name
        self.num_workers = num_workers
        self.sample_rate = 16000
        self.chunk_size = 1024
        
        # PyAudioåˆæœŸåŒ–
        self.p = pyaudio.PyAudio()
        
        # ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ãƒãƒƒãƒ•ã‚¡
        self.buffer = MultiLevelBuffer(self.sample_rate)
        
        # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®ã‚­ãƒ¥ãƒ¼
        self.task_queue = mp.Queue(maxsize=10)
        self.result_queue = mp.Queue()
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹
        self.workers = []
        
        # çµæœç®¡ç†
        self.results_by_level = {
            'short': deque(maxlen=10),
            'medium': deque(maxlen=5),
            'long': deque(maxlen=2)
        }
        
        self.is_running = False
        
    def start_workers(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•"""
        for i in range(self.num_workers):
            worker = TranscriptionWorker(
                self.model_name,
                self.task_queue,
                self.result_queue
            )
            worker.start()
            self.workers.append(worker)
            
    def stop_workers(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢"""
        for _ in self.workers:
            self.task_queue.put(None)
            
        for worker in self.workers:
            worker.join(timeout=5)
            if worker.is_alive():
                worker.terminate()
                
    def save_audio_chunk(self, chunk: AudioChunk) -> str:
        """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜"""
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_filename = tmp_file.name
            
        with wave.open(tmp_filename, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(self.sample_rate)
            wf.writeframes(chunk.audio.astype(np.int16).tobytes())
            
        return tmp_filename
        
    def recording_thread(self):
        """éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰"""
        stream = self.p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size
        )
        
        print("\nğŸ¤ æ”¹è‰¯ç‰ˆãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«éŒ²éŸ³ã‚’é–‹å§‹... (Ctrl+Cã§çµ‚äº†)")
        print("ğŸ“Š ãƒ¬ãƒ™ãƒ«: short(2ç§’) / medium(5ç§’) / long(15ç§’)")
        print("ğŸ”‡ ç„¡éŸ³ã¯è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
        print("-" * 80)
        
        try:
            while self.is_running:
                data = stream.read(self.chunk_size, exception_on_overflow=False)
                audio_chunk = np.frombuffer(data, dtype=np.int16)
                
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                self.buffer.add_audio(audio_chunk)
                
                # å‡¦ç†ã™ã¹ããƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºèª
                chunks_to_process = self.buffer.get_chunks_to_process()
                
                for chunk in chunks_to_process:
                    audio_file = self.save_audio_chunk(chunk)
                    
                    # å‰ã®çµæœã‹ã‚‰æ–‡è„ˆã‚’å–å¾—ï¼ˆèª¤èªè­˜ã¯é™¤å¤–ï¼‰
                    prompt = self.get_clean_context(chunk.level)
                    
                    try:
                        self.task_queue.put_nowait({
                            'audio_file': audio_file,
                            'level': chunk.level,
                            'timestamp': chunk.timestamp,
                            'duration': chunk.duration,
                            'prompt': prompt,
                            'rms': chunk.rms
                        })
                        print(f"\rğŸ“¤ [{chunk.level}] ãƒãƒ£ãƒ³ã‚¯é€ä¿¡ (RMS: {chunk.rms:.0f})", end="", flush=True)
                    except queue.Full:
                        print(f"\nâš ï¸ ã‚­ãƒ¥ãƒ¼ãŒæº€æ¯ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        os.unlink(audio_file)
                        
        finally:
            stream.stop_stream()
            stream.close()
            
    def get_clean_context(self, level: str) -> Optional[str]:
        """ã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡è„ˆã‚’å–å¾—ï¼ˆèª¤èªè­˜ã‚’é™¤å¤–ï¼‰"""
        context_map = {
            'medium': ('short', 100),
            'long': ('medium', 200)
        }
        
        if level in context_map:
            source_level, max_chars = context_map[level]
            if self.results_by_level[source_level]:
                # æœ€æ–°ã®æœ‰åŠ¹ãªçµæœã‚’æ¢ã™
                for result in reversed(self.results_by_level[source_level]):
                    if result['text'] and result.get('no_speech_prob', 0) < 0.5:
                        return result['text'][-max_chars:]
        return None
        
    def result_handler_thread(self):
        """çµæœå‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰"""
        while self.is_running or not self.result_queue.empty():
            try:
                result = self.result_queue.get(timeout=1)
                
                # æœ‰åŠ¹ãªçµæœã®ã¿ä¿å­˜ãƒ»è¡¨ç¤º
                if result['text']:
                    self.results_by_level[result['level']].append(result)
                    self.display_result(result)
                    
            except queue.Empty:
                continue
                
    def display_result(self, result):
        """çµæœã‚’è¡¨ç¤º"""
        level_emoji = {
            'short': 'ğŸƒ',
            'medium': 'ğŸš¶',
            'long': 'ğŸ¯'
        }
        
        emoji = level_emoji.get(result['level'], 'â“')
        speed = result['duration'] / result['transcribe_time']
        
        print(f"\n{emoji} [{result['level']:6}] {result['text']}")
        print(f"   éŸ³å£°: {result['duration']:.1f}ç§’ / å‡¦ç†: {result['transcribe_time']:.1f}ç§’ (é€Ÿåº¦: {speed:.1f}å€) / RMS: {result['rms']:.0f}")
        
        if result['level'] == 'long' and self.results_by_level['medium']:
            print("   ğŸ“ˆ ç²¾åº¦å‘ä¸Š: ã‚ˆã‚Šé•·ã„æ–‡è„ˆã§å†èªè­˜ã—ã¾ã—ãŸ")
            
        print("-" * 80)
        
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        self.is_running = True
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•
        self.start_workers()
        
        # å„ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•
        record_thread = threading.Thread(target=self.recording_thread)
        record_thread.daemon = True
        record_thread.start()
        
        result_thread = threading.Thread(target=self.result_handler_thread)
        result_thread.daemon = True
        result_thread.start()
        
        try:
            while True:
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ çµ‚äº†å‡¦ç†ä¸­...")
            self.is_running = False
            
            record_thread.join(timeout=2)
            result_thread.join(timeout=5)
            self.stop_workers()
            
            print("âœ… çµ‚äº†ã—ã¾ã—ãŸ")
            
        finally:
            self.p.terminate()

def main():
    print("=== Whisper ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«éŸ³å£°èªè­˜ v2 ===")
    print("ç„¡éŸ³æ¤œå‡ºã¨èª¤èªè­˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ã")
    
    model_name = "small"
    num_workers = 2
    
    print(f"\nè¨­å®š:")
    print(f"- ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"- ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {num_workers}")
    print(f"- è‡ªå‹•ç„¡éŸ³ã‚¹ã‚­ãƒƒãƒ—: ON")
    print(f"- èª¤èªè­˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: ON")
    
    transcriber = MultiLevelTranscriber(
        model_name=model_name,
        num_workers=num_workers
    )
    
    transcriber.run()

if __name__ == "__main__":
    main()