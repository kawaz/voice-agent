#!/usr/bin/env python3
import whisper
import pyaudio
import numpy as np
import wave
import tempfile
import os
import time
import threading
import queue
from collections import deque

class RealtimeTranscriber:
    def __init__(self, model_name="small", silence_threshold=250, silence_duration=1.2, chunk_duration=5.0):
        """
        ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        
        Args:
            model_name: Whisperãƒ¢ãƒ‡ãƒ«å
            silence_threshold: ç„¡éŸ³åˆ¤å®šã®é–¾å€¤
            silence_duration: ç„¡éŸ³ç¶™ç¶šæ™‚é–“ï¼ˆç§’ï¼‰ã§éŒ²éŸ³ã‚’åŒºåˆ‡ã‚‹
            chunk_duration: æœ€å¤§éŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰- ã“ã®æ™‚é–“ã§å¼·åˆ¶çš„ã«åŒºåˆ‡ã‚‹
        """
        print(f"ğŸ¤– Whisperãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self.model = whisper.load_model(model_name)
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
        
        self.sample_rate = 16000
        self.chunk_size = 1024
        self.silence_threshold = silence_threshold
        self.silence_duration = silence_duration
        self.chunk_duration = chunk_duration
        
        # PyAudioåˆæœŸåŒ–
        self.p = pyaudio.PyAudio()
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¥ãƒ¼
        self.audio_queue = queue.Queue()
        self.is_running = False
        
        # éŸ³é‡ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆ
        self.volume_history = deque(maxlen=50)
        
    def calculate_rms(self, data):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®RMSï¼ˆRoot Mean Squareï¼šäºŒä¹—å¹³å‡å¹³æ–¹æ ¹ï¼‰ã‚’è¨ˆç®—"""
        try:
            audio_data = np.frombuffer(data, dtype=np.int16)
            if len(audio_data) == 0:
                return 0
            mean_square = np.mean(audio_data.astype(np.float32)**2)
            if mean_square < 0:
                return 0
            return np.sqrt(mean_square)
        except:
            return 0
    
    def print_volume_meter(self, rms, is_recording=False):
        """éŸ³é‡ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¡¨ç¤º"""
        max_level = 50
        level = min(int(rms / 100), max_level)
        bar = "â–ˆ" * level + "â–‘" * (max_level - level)
        
        if is_recording:
            status = "ğŸ”´ éŒ²éŸ³ä¸­..."
        else:
            status = "ğŸ”‡ ç„¡éŸ³" if rms < self.silence_threshold else "ğŸ”Š éŸ³å£°æ¤œå‡º"
        
        print(f"\réŸ³é‡: [{bar}] RMS: {rms:>6.0f} {status}   ", end="", flush=True)
    
    def get_audio_duration(self, frames):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’ç§’å˜ä½ã§è¨ˆç®—"""
        total_samples = len(frames) * self.chunk_size
        duration = total_samples / self.sample_rate
        return duration
    
    def record_audio_chunk(self):
        """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’éŒ²éŸ³ã—ã¦ç„¡éŸ³æ¤œå‡º"""
        stream = self.p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size
        )
        
        print("\nğŸ¤ éŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã™... (Ctrl+Cã§çµ‚äº†)")
        print("è©±ã™ã¨è‡ªå‹•çš„ã«èªè­˜ã•ã‚Œã¾ã™")
        print(f"RMS = Root Mean Squareï¼ˆäºŒä¹—å¹³å‡å¹³æ–¹æ ¹ï¼‰: éŸ³é‡ãƒ¬ãƒ™ãƒ«ã®æŒ‡æ¨™")
        print("-" * 80)
        
        frames = []
        silence_chunks = 0
        chunks_per_second = self.sample_rate / self.chunk_size
        silence_chunks_needed = int(self.silence_duration * chunks_per_second)
        max_chunks = int(self.chunk_duration * chunks_per_second)
        is_speaking = False
        recording_start_time = None
        
        try:
            while self.is_running:
                data = stream.read(self.chunk_size, exception_on_overflow=False)
                rms = self.calculate_rms(data)
                
                # éŸ³é‡å±¥æ­´ã‚’è¨˜éŒ²
                self.volume_history.append(rms)
                
                # éŸ³å£°æ¤œå‡º
                if rms > self.silence_threshold:
                    if not is_speaking:
                        print(f"\nğŸ”Š éŸ³å£°æ¤œå‡ºï¼éŒ²éŸ³ä¸­... (RMS: {rms:.0f})")
                        is_speaking = True
                        recording_start_time = time.time()
                    frames.append(data)
                    silence_chunks = 0
                    
                    # æœ€å¤§éŒ²éŸ³æ™‚é–“ãƒã‚§ãƒƒã‚¯
                    if len(frames) >= max_chunks:
                        print(f"\nâ±ï¸ æœ€å¤§éŒ²éŸ³æ™‚é–“ï¼ˆ{self.chunk_duration}ç§’ï¼‰ã«é”ã—ã¾ã—ãŸ")
                        self._process_audio(frames, recording_start_time)
                        frames = []
                        silence_chunks = 0
                        is_speaking = False
                        recording_start_time = None
                    
                elif is_speaking:
                    # è©±ã—ã¦ã„ã‚‹æœ€ä¸­ã®ç„¡éŸ³
                    frames.append(data)
                    silence_chunks += 1
                    
                    # ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰éŒ²éŸ³çµ‚äº†
                    if silence_chunks > silence_chunks_needed:
                        print(" âœ… éŒ²éŸ³å®Œäº†ï¼")
                        self._process_audio(frames, recording_start_time)
                        frames = []
                        silence_chunks = 0
                        is_speaking = False
                        recording_start_time = None
                else:
                    # éŒ²éŸ³ã—ã¦ã„ãªã„æ™‚ã¯éŸ³é‡ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¡¨ç¤º
                    self.print_volume_meter(rms, is_recording=False)
                        
        finally:
            stream.stop_stream()
            stream.close()
    
    def _process_audio(self, frames, start_time):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
        if len(frames) > self.sample_rate / self.chunk_size:  # 1ç§’ä»¥ä¸Šã®éŸ³å£°ã®ã¿
            audio_duration = self.get_audio_duration(frames)
            self.audio_queue.put((frames.copy(), audio_duration, start_time))
            
            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
            if self.volume_history:
                avg_volume = np.mean(list(self.volume_history))
                max_volume = max(self.volume_history)
                print(f"ğŸ“Š éŸ³é‡çµ±è¨ˆ - å¹³å‡: {avg_volume:.0f}, æœ€å¤§: {max_volume:.0f}")
                print(f"â±ï¸  éŒ²éŸ³æ™‚é–“: {audio_duration:.1f}ç§’")
                print("-" * 80)
    
    def save_audio(self, frames):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_filename = tmp_file.name
            
        with wave.open(tmp_filename, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(self.p.get_sample_size(pyaudio.paInt16))
            wf.setframerate(self.sample_rate)
            wf.writeframes(b''.join(frames))
            
        return tmp_filename
    
    def transcribe_worker(self):
        """æ–‡å­—èµ·ã“ã—ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        while self.is_running or not self.audio_queue.empty():
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                frames, audio_duration, start_time = self.audio_queue.get(timeout=1)
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                audio_file = self.save_audio(frames)
                
                try:
                    # æ–‡å­—èµ·ã“ã—
                    print("ğŸ¤” èªè­˜ä¸­...", end="", flush=True)
                    transcribe_start = time.time()
                    result = self.model.transcribe(audio_file, language="ja", fp16=False)
                    transcribe_time = time.time() - transcribe_start
                    
                    # çµæœè¡¨ç¤º
                    text = result["text"].strip()
                    if text:
                        print(f"\rğŸ“ èªè­˜çµæœ: {text}")
                        print(f"   éŸ³å£°é•·: {audio_duration:.1f}ç§’ / å‡¦ç†æ™‚é–“: {transcribe_time:.1f}ç§’ (é€Ÿåº¦: {audio_duration/transcribe_time:.1f}å€)")
                        
                        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚‚è¡¨ç¤ºï¼ˆé•·ã„éŸ³å£°ã®å ´åˆï¼‰
                        if audio_duration > 10 and len(result["segments"]) > 1:
                            print("   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ:")
                            for seg in result["segments"][:5]:  # æœ€åˆã®5ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¾ã§
                                print(f"   [{seg['start']:.1f}s-{seg['end']:.1f}s] {seg['text']}")
                            if len(result["segments"]) > 5:
                                print(f"   ... ä»– {len(result["segments"])-5} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                        
                        print("-" * 80)
                    else:
                        print("\râ“ èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                        print("-" * 80)
                        
                except Exception as e:
                    if self.is_running:
                        print(f"\nèªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                finally:
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
                    if os.path.exists(audio_file):
                        os.unlink(audio_file)
                        
            except queue.Empty:
                continue
            except Exception as e:
                if self.is_running:
                    print(f"\nå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def run(self):
        """é€£ç¶šéŸ³å£°èªè­˜ã‚’é–‹å§‹"""
        self.is_running = True
        
        # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        record_thread = threading.Thread(target=self.record_audio_chunk)
        record_thread.daemon = True
        record_thread.start()
        
        # æ–‡å­—èµ·ã“ã—ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        transcribe_thread = threading.Thread(target=self.transcribe_worker)
        transcribe_thread.daemon = True
        transcribe_thread.start()
        
        try:
            # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§å¾…æ©Ÿ
            while True:
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ çµ‚äº†ä¸­...")
            self.is_running = False
            
            # ã‚¹ãƒ¬ãƒƒãƒ‰ã®çµ‚äº†ã‚’å¾…ã¤
            record_thread.join(timeout=2)
            
            # æ®‹ã‚Šã®ã‚­ãƒ¥ãƒ¼ã‚’å‡¦ç†
            if not self.audio_queue.empty():
                print("æ®‹ã‚Šã®éŸ³å£°ã‚’å‡¦ç†ä¸­...")
            transcribe_thread.join(timeout=10)
            
            print("âœ… çµ‚äº†ã—ã¾ã—ãŸ")
            
        finally:
            self.p.terminate()

def main():
    print("=== Whisper ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ï¼ˆæ”¹è‰¯ç‰ˆï¼‰===")
    print("é•·ã„éŸ³å£°ã‚‚æœ€å¤§5ç§’ã§åŒºåˆ‡ã£ã¦é †æ¬¡èªè­˜ã—ã¾ã™")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    model_name = "small"
    silence_threshold = 250
    silence_duration = 1.2
    chunk_duration = 5.0  # æœ€å¤§éŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
    
    print(f"\nè¨­å®š:")
    print(f"- ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"- ç„¡éŸ³é–¾å€¤: {silence_threshold}")
    print(f"- ç„¡éŸ³ç¶™ç¶šæ™‚é–“: {silence_duration}ç§’")
    print(f"- æœ€å¤§éŒ²éŸ³æ™‚é–“: {chunk_duration}ç§’")
    
    # é€£ç¶šèªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•
    transcriber = RealtimeTranscriber(
        model_name=model_name,
        silence_threshold=silence_threshold,
        silence_duration=silence_duration,
        chunk_duration=chunk_duration
    )
    
    transcriber.run()

if __name__ == "__main__":
    main()